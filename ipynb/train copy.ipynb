{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_built(),torch.backends.mps.is_available(),torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = torch.device('mps')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memory usage?\n",
    "flops\n",
    "reproduce experiments\n",
    "ablation study\n",
    "what else except attention/ architecture properties / mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/Desktop/void-diffusion/env/lib/python3.11/site-packages/datasets/load.py:1461: FutureWarning: The repository for wmt14 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wmt14\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wmt14\", 'de-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") # split + toInt\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/Desktop/void-diffusion/env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "from transformer import Transformer\n",
    "from transformerModel import TransformerModel\n",
    "\n",
    "src_vocab_size = tokenizer.vocab_size\n",
    "tgt_vocab_size = tokenizer.vocab_size\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 512\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
    "\n",
    "transformer = TransformerModel(src_vocab_size, d_model, num_heads, d_ff, num_layers, dropout).to(device)\n",
    "\n",
    "# # Generate random sample data\n",
    "# src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "# tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in dataset['train'].iter(batch_size):\n",
    "#     translation = batch['translation']\n",
    "    \n",
    "#     source = [sample['de'] for sample in translation]\n",
    "#     target = [sample['en'] for sample in translation]\n",
    "#     source_tokenized = tokenizer(source, padding='max_length',max_length=max_seq_length)\n",
    "#     target_tokenized = tokenizer(target, padding='max_length',max_length=max_seq_length)\n",
    "    \n",
    "#     source_tensor = torch.tensor(source_tokenized['input_ids'], dtype=torch.long)\n",
    "#     target_tensor = torch.tensor(target_tokenized['input_ids'], dtype=torch.long)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(batch):\n",
    "    translation = batch['translation']\n",
    "    source = [sample['de'] for sample in translation]\n",
    "    target = [sample['en'] for sample in translation]\n",
    "    source_tokenized = tokenizer(source, padding='max_length',max_length=max_seq_length)\n",
    "    target_tokenized = tokenizer(target, padding='max_length',max_length=max_seq_length)\n",
    "    \n",
    "    source_tensor = torch.tensor(source_tokenized['input_ids'], dtype=torch.long).to(device)\n",
    "    target_tensor = torch.tensor(target_tokenized['input_ids'], dtype=torch.long).to(device)\n",
    "    \n",
    "    return source_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data,tgt_data = tokenization(dataset['train'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 50257]) torch.Size([512, 50257]) torch.Size([1, 512])\n",
      "Epoch: 1, Loss: 10.253396034240723 Time:1711615665.5115662\n",
      "torch.Size([1, 512, 50257]) torch.Size([512, 50257]) torch.Size([1, 512])\n",
      "Epoch: 2, Loss: 2.154623508453369 Time:1711615665.7089438\n",
      "torch.Size([1, 512, 50257]) torch.Size([512, 50257]) torch.Size([1, 512])\n",
      "Epoch: 3, Loss: 0.18717262148857117 Time:1711615665.9166348\n",
      "torch.Size([1, 512, 50257]) torch.Size([512, 50257]) torch.Size([1, 512])\n",
      "Epoch: 4, Loss: 0.12320186197757721 Time:1711615666.136462\n",
      "torch.Size([1, 512, 50257]) torch.Size([512, 50257]) torch.Size([1, 512])\n",
      "Epoch: 5, Loss: 0.12956474721431732 Time:1711615666.394814\n",
      "torch.Size([1, 512, 50257]) torch.Size([512, 50257]) torch.Size([1, 512])\n",
      "Epoch: 6, Loss: 0.14241713285446167 Time:1711615666.59692\n",
      "torch.Size([1, 512, 50257]) torch.Size([512, 50257]) torch.Size([1, 512])\n",
      "Epoch: 7, Loss: 0.1398051381111145 Time:1711615666.8353338\n",
      "torch.Size([1, 512, 50257]) torch.Size([512, 50257]) torch.Size([1, 512])\n",
      "Epoch: 8, Loss: 0.14459356665611267 Time:1711615667.065313\n",
      "torch.Size([1, 512, 50257]) torch.Size([512, 50257]) torch.Size([1, 512])\n",
      "Epoch: 9, Loss: 0.14152024686336517 Time:1711615667.2795289\n",
      "torch.Size([1, 512, 50257]) torch.Size([512, 50257]) torch.Size([1, 512])\n",
      "Epoch: 10, Loss: 0.13683441281318665 Time:1711615667.474496\n",
      "torch.Size([1, 512, 50257]) torch.Size([512, 50257]) torch.Size([1, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Time:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data)\n",
    "    output_flat = output.view(-1, src_vocab_size)\n",
    "    print(output.shape, output_flat.shape,tgt_data.shape)\n",
    "    \n",
    "    loss = criterion(output_flat, tgt_data.contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()} Time:{time.time()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "\n",
    "# Generate random sample validation data\n",
    "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
