{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# from transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False, True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_built(),torch.backends.mps.is_available(),torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = torch.device('mps')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memory usage? different batch size?\n",
    "label smoothing\n",
    "beam search\n",
    "Model Regularization via Label Smoothing\n",
    "bleu\n",
    "flops\n",
    "reproduce experiments\n",
    "ablation study\n",
    "what else except attention/ architecture properties / mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ubuntu/7ACA674ACA67022D/void-diffusion/env/lib/python3.11/site-packages/datasets/load.py:1461: FutureWarning: The repository for wmt14 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wmt14\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885e9e62808f40a6b07afd8fbccdb759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wmt14\",\"de-en\")\n",
    "dataset2 = load_dataset(\"wmt14\",\"fr-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") # split + toInt\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer\n",
    "\n",
    "src_vocab_size = tokenizer.vocab_size\n",
    "tgt_vocab_size = tokenizer.vocab_size\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 512\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
    "\n",
    "# # Generate random sample data\n",
    "# src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "# tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in dataset['train'].iter(batch_size):\n",
    "#     translation = batch['translation']\n",
    "    \n",
    "#     source = [sample['de'] for sample in translation]\n",
    "#     target = [sample['en'] for sample in translation]\n",
    "#     source_tokenized = tokenizer(source, padding='max_length',max_length=max_seq_length)\n",
    "#     target_tokenized = tokenizer(target, padding='max_length',max_length=max_seq_length)\n",
    "    \n",
    "#     source_tensor = torch.tensor(source_tokenized['input_ids'], dtype=torch.long)\n",
    "#     target_tensor = torch.tensor(target_tokenized['input_ids'], dtype=torch.long)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(batch):\n",
    "    translation = batch['translation']\n",
    "    source = [sample['de'] for sample in translation]\n",
    "    target = [sample['en'] for sample in translation]\n",
    "    source_tokenized = tokenizer(source, padding='max_length',max_length=max_seq_length)\n",
    "    target_tokenized = tokenizer(target, padding='max_length',max_length=max_seq_length)\n",
    "    \n",
    "    source_tensor = torch.tensor(source_tokenized['input_ids'], dtype=torch.long).to(device)\n",
    "    target_tensor = torch.tensor(target_tokenized['input_ids'], dtype=torch.long).to(device)\n",
    "    \n",
    "    return source_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data,tgt_data = tokenization(dataset['train'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {'translation': [{'de': 'Wiederaufnahme der Sitzungsperiode', 'en': 'Resumption of the session'}]})\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(dataset['train'].iter(1)):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-03-31 14:34:23 25247:25247 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu cpu cpu\n",
      "Batch: 1, Loss: 11.031022071838379 Time:28.594189882278442\n",
      "cpu cpu cpu\n",
      "Batch: 2, Loss: 3.0969715118408203 Time:56.324806928634644\n",
      "cpu cpu cpu\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "# total_loss = 0\n",
    "batch_size = 24\n",
    "start = time.time()\n",
    "with profile(activities=[ProfilerActivity.CPU],\n",
    "        profile_memory=True, record_shapes=True) as prof:\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i, batch in enumerate(dataset['train'].iter(batch_size)):\n",
    "            src_data, tgt_data= tokenization(batch)\n",
    "            output = transformer(src_data, tgt_data[:, :-1])\n",
    "            loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(transformer.parameters(), 0.5) # ??????\n",
    "            optimizer.step()\n",
    "            \n",
    "            # total_loss += loss.item()\n",
    "            print(f\"Batch: {i+1}, Loss: {loss.item()} Time:{time.time() - start}\")\n",
    "            \n",
    "            \n",
    "        print(f\"Epoch: {epoch+1}, Loss: {loss.item()} Time:{time.time() - start }\")\n",
    "        break\n",
    "\n",
    "# print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "# print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "\n",
    "# Generate random sample validation data\n",
    "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
