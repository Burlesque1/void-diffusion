{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='mps'), True, True, False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = torch.device('mps')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "device,torch.backends.mps.is_built(),torch.backends.mps.is_available(),torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/bytedance/.cache/huggingface/modules/datasets_modules/datasets/wmt14/d5cfc45c32d826941d8678bf74c810c2aaa057cdc5544f1e23a5dab8c0407a9f (last modified on Tue Mar 26 14:25:53 2024) since it couldn't be found locally at wmt14, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /Users/bytedance/.cache/huggingface/modules/datasets_modules/datasets/wmt14/d5cfc45c32d826941d8678bf74c810c2aaa057cdc5544f1e23a5dab8c0407a9f (last modified on Tue Mar 26 14:25:53 2024) since it couldn't be found locally at wmt14, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5ce6c41aa94ea8926cff95db6c2467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wmt14\",\"de-en\")\n",
    "dataset2 = load_dataset(\"wmt14\",\"fr-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") # split + toInt\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/Desktop/void-diffusion/env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = tokenizer.vocab_size\n",
    "tgt_vocab_size = tokenizer.vocab_size\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 512\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "transformer_model = nn.Transformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/Desktop/void-diffusion/env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "a = transformer_model.state_dict()\n",
    "b = transformer_model = nn.Transformer().state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0286, -0.0263, -0.0104,  ..., -0.0047,  0.0306,  0.0273],\n",
       "         [-0.0290,  0.0336, -0.0040,  ..., -0.0415,  0.0205,  0.0471],\n",
       "         [-0.0144,  0.0240, -0.0361,  ..., -0.0434,  0.0185,  0.0242],\n",
       "         ...,\n",
       "         [ 0.0149,  0.0396,  0.0207,  ..., -0.0514, -0.0487, -0.0321],\n",
       "         [-0.0105, -0.0482,  0.0078,  ..., -0.0471, -0.0050,  0.0249],\n",
       "         [-0.0057, -0.0237, -0.0416,  ...,  0.0181,  0.0031,  0.0225]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 0.0204,  0.0524,  0.0104,  ...,  0.0445, -0.0244,  0.0377],\n",
       "         [ 0.0054, -0.0325, -0.0534,  ...,  0.0420,  0.0412,  0.0110],\n",
       "         [ 0.0274,  0.0496,  0.0317,  ..., -0.0135,  0.0325,  0.0316],\n",
       "         ...,\n",
       "         [-0.0463, -0.0424,  0.0257,  ...,  0.0400,  0.0167,  0.0172],\n",
       "         [-0.0183,  0.0436,  0.0078,  ..., -0.0442,  0.0299,  0.0029],\n",
       "         [-0.0170,  0.0439, -0.0067,  ...,  0.0195,  0.0503,  0.0076]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['encoder.layers.0.self_attn.in_proj_weight'],b['encoder.layers.0.self_attn.in_proj_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(batch):\n",
    "    translation = batch['translation']\n",
    "    source = [sample['en'] for sample in translation]\n",
    "    target = [sample['fr'] for sample in translation]\n",
    "    source_tokenized = tokenizer(source, padding='max_length',max_length=max_seq_length)\n",
    "    target_tokenized = tokenizer(target, padding='max_length',max_length=max_seq_length)\n",
    "    \n",
    "    source_tensor = torch.tensor(source_tokenized['input_ids'], dtype=torch.float).to(device)\n",
    "    target_tensor = torch.tensor(target_tokenized['input_ids'], dtype=torch.float).to(device)\n",
    "    \n",
    "    return source_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1907, -2.9960,  1.1793,  ...,  0.5404, -0.8455, -1.9491],\n",
      "        [-0.0207, -2.9981,  0.9145,  ...,  0.6965, -0.6856, -1.7537],\n",
      "        [-1.0192, -2.5926,  0.6574,  ...,  0.3512, -1.2069, -1.3054],\n",
      "        ...,\n",
      "        [ 0.6451, -3.1944,  1.2302,  ...,  0.0880, -0.5729, -0.9669],\n",
      "        [-1.0712, -2.8826,  0.7981,  ...,  0.6596, -0.7993, -0.9286],\n",
      "        [-0.8379, -3.0235,  1.6540,  ...,  0.2526, -0.9191, -1.0314]],\n",
      "       device='mps:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataset2['train'].iter(batch_size)):\n",
    "    src,tgt = tokenization(batch)\n",
    "    print(transformer_model(src, tgt))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5906e+08, device='mps:0', grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = transformer_model(src, tgt)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(o,tgt)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8408964276313782"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "candidate_corpus = [['My', 'full', 'pytorch', 'test'], ['Another', 'Sentence']]\n",
    "references_corpus = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']]]\n",
    "\n",
    "bleu_score(candidate_corpus, references_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0)  # 假设0是teacher_forcing_ratio\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "# total_loss = 0\n",
    "batch_size = 24\n",
    "start = time.time()\n",
    "with profile(activities=[ProfilerActivity.CPU],\n",
    "        profile_memory=True, record_shapes=True) as prof:\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i, batch in enumerate(dataset['train'].iter(batch_size)):\n",
    "            src_data, tgt_data= tokenization(batch)\n",
    "            output = transformer(src_data, tgt_data[:, :-1])\n",
    "            loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(transformer.parameters(), 0.5) # ??????\n",
    "            optimizer.step()\n",
    "            \n",
    "            # total_loss += loss.item()\n",
    "            print(f\"Batch: {i+1}, Loss: {loss.item()} Time:{time.time() - start}\")\n",
    "            \n",
    "            \n",
    "        print(f\"Epoch: {epoch+1}, Loss: {loss.item()} Time:{time.time() - start }\")\n",
    "        break\n",
    "\n",
    "# print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "# print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/seq2seq_transformer/seq2seq_transformer.py\n",
    "https://github.com/gordicaleksa/pytorch-original-transformer\n",
    "https://zhuanlan.zhihu.com/p/581334630"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
