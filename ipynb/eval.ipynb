{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), False, False, True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = torch.device('mps')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "device,torch.backends.mps.is_built(),torch.backends.mps.is_available(),torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/wmt14/d5cfc45c32d826941d8678bf74c810c2aaa057cdc5544f1e23a5dab8c0407a9f (last modified on Sun Mar 31 14:29:52 2024) since it couldn't be found locally at wmt14, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bcea8b917fa42199f862ed53cc926a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# dataset = load_dataset(\"wmt14\",\"de-en\")\n",
    "dataset2 = load_dataset(\"wmt14\",\"fr-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ubuntu/7ACA674ACA67022D/void-diffusion/env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = tokenizer.vocab_size\n",
    "tgt_vocab_size = tokenizer.vocab_size\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 512\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "transformer_model = nn.Transformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model(src, tgt).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") # split + toInt\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(batch):\n",
    "    translation = batch['translation']\n",
    "    source = [sample['en'] for sample in translation]\n",
    "    target = [sample['fr'] for sample in translation]\n",
    "    source_tokenized = tokenizer(source, padding='max_length',max_length=max_seq_length)\n",
    "    target_tokenized = tokenizer(target, padding='max_length',max_length=max_seq_length)\n",
    "    \n",
    "    source_tensor = torch.tensor(source_tokenized['input_ids'], dtype=torch.float).to(device)\n",
    "    target_tensor = torch.tensor(target_tokenized['input_ids'], dtype=torch.float).to(device)\n",
    "    \n",
    "    return source_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.9650e+03, 2.4098e+04, 2.8600e+02,  ..., 5.0256e+04, 5.0256e+04,\n",
      "         5.0256e+04],\n",
      "        [4.0000e+01, 1.3627e+04, 2.8291e+04,  ..., 5.0256e+04, 5.0256e+04,\n",
      "         5.0256e+04],\n",
      "        [7.0030e+03, 1.1000e+01, 3.5500e+02,  ..., 5.0256e+04, 5.0256e+04,\n",
      "         5.0256e+04],\n",
      "        ...,\n",
      "        [1.7220e+03, 6.6100e+02, 4.2300e+02,  ..., 5.0256e+04, 5.0256e+04,\n",
      "         5.0256e+04],\n",
      "        [1.8580e+03, 3.1800e+02, 1.1000e+01,  ..., 5.0256e+04, 5.0256e+04,\n",
      "         5.0256e+04],\n",
      "        [1.1350e+03, 4.6600e+02, 4.0700e+02,  ..., 5.0256e+04, 5.0256e+04,\n",
      "         5.0256e+04]], device='cuda:0') tensor([[6.2070e+03, 1.7163e+04, 3.9000e+02,  ..., 5.0256e+04, 5.0256e+04,\n",
      "         5.0256e+04],\n",
      "        [4.0932e+04, 3.9073e+04, 5.6500e+02,  ..., 5.0256e+04, 5.0256e+04,\n",
      "         5.0256e+04],\n",
      "        [5.3770e+03, 1.3260e+03, 4.1000e+02,  ..., 5.0256e+04, 5.0256e+04,\n",
      "         5.0256e+04],\n",
      "        ...,\n",
      "        [3.5000e+01, 6.0000e+00, 1.4272e+04,  ..., 5.0256e+04, 5.0256e+04,\n",
      "         5.0256e+04],\n",
      "        [3.3666e+04, 3.3100e+02, 2.5700e+02,  ..., 5.0256e+04, 5.0256e+04,\n",
      "         5.0256e+04],\n",
      "        [4.5000e+01, 5.1600e+02, 4.9700e+02,  ..., 5.0256e+04, 5.0256e+04,\n",
      "         5.0256e+04]], device='cuda:0') torch.Size([64, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ubuntu/7ACA674ACA67022D/void-diffusion/env/lib/python3.11/site-packages/torch/nn/functional.py:5527: UserWarning: 1Torch was not compiled with memory efficient attention. (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:505.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataset2['train'].iter(batch_size)):\n",
    "    src,tgt = tokenization(batch)\n",
    "    print(src,tgt,transformer_model(src, tgt).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/seq2seq_transformer/seq2seq_transformer.py\n",
    "https://github.com/gordicaleksa/pytorch-original-transformer\n",
    "https://zhuanlan.zhihu.com/p/581334630"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
